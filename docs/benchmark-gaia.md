# üß™ GAIA Benchmark & Agent Reasoning  
> Benchmark GAIA e Racioc√≠nio do Agente

---

## EN ‚Äì What is the GAIA Benchmark?

**GAIA** (General AI Agent Assessment) is an advanced benchmark designed to evaluate autonomous reasoning, planning, memory use, and tool execution in intelligent agents.  
It simulates realistic tasks, with multi-step goals that require contextual understanding and tool orchestration.

Although GAIA is not open-source, this project includes a **simulated version**, inspired by its logic, to allow local testing and training.

---

### üéØ Educational Objectives

- Train agents to decompose tasks and reason with context
- Integrate memory and tools dynamically
- Benchmark output accuracy with a scoring function
- Compare agents and configurations across test cases

---

### ‚öôÔ∏è Simulation Components

| Module               | Description                                                  |
|----------------------|--------------------------------------------------------------|
| `agent_self_reflection.py` | Reasoning loop with self-evaluation                     |
| `prompt-strategy.md`      | Dynamic prompt selection, fallback logic                |
| `multimodal_task_runner.py` | Handles image + document queries                     |
| `test_benchmark_score.py`  | Simulated benchmark with scoring                       |

---

## PT ‚Äì O que √© o Benchmark GAIA?

**GAIA** (Avalia√ß√£o Geral de Agentes de IA) √© um benchmark avan√ßado projetado para avaliar o racioc√≠nio aut√¥nomo, o planejamento, o uso de mem√≥ria e a execu√ß√£o de ferramentas por agentes inteligentes.  
Ele simula tarefas realistas com m√∫ltiplas etapas, exigindo compreens√£o contextual e coordena√ß√£o de ferramentas.

Embora o GAIA n√£o seja de c√≥digo aberto, este projeto inclui uma **vers√£o simulada** inspirada em sua l√≥gica, permitindo testes e treinamentos locais.

---

### üéØ Objetivos Educacionais

- Treinar agentes para decompor tarefas e raciocinar com contexto
- Integrar mem√≥ria e ferramentas dinamicamente
- Avaliar a precis√£o das respostas com fun√ß√£o de pontua√ß√£o
- Comparar agentes e configura√ß√µes em diferentes testes

---

### ‚öôÔ∏è Componentes da Simula√ß√£o

| M√≥dulo                    | Descri√ß√£o                                                  |
|---------------------------|------------------------------------------------------------|
| `agent_self_reflection.py` | Loop de racioc√≠nio com autoavalia√ß√£o                      |
| `prompt-strategy.md`       | Sele√ß√£o din√¢mica de prompt e l√≥gica de fallback           |
| `multimodal_task_runner.py` | Consulta multimodal: imagem + documento                   |
| `test_benchmark_score.py`   | Benchmark simulado com sistema de pontua√ß√£o              |

---

üìÇ See also:  
- [Architecture Overview](architecture.md)  
- [Prompt Strategy](prompts-strategy.md)  
- [Vision Pipeline](multimodal-vision.md)

---
